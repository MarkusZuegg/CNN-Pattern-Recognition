{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import pytorch_lightning as pl\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Subset, TensorDataset\n",
    "from torch.optim import SGD\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchmetrics\n",
    "import torchvision\n",
    "from torchvision.transforms import Compose, ToTensor\n",
    "from torch import flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIFAR10_mod(pl.LightningModule):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.classes = num_classes\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3)\n",
    "        self.conv2 = nn.Conv2d(32, 32, 3)\n",
    "        self.fc1 = nn.Linear(1152, 500)\n",
    "        self.fc2 = nn.Linear(500, self.classes)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.mxpool1 = nn.MaxPool2d(2)\n",
    "        self.mxpool2 = nn.MaxPool2d(2)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        self.accuracy = torchmetrics.classification.Accuracy(task=\"multiclass\", num_classes=self.classes)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.mxpool1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.mxpool2(x)\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu3(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        output = self.softmax(x)\n",
    "        return output\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        LR = 1e-3\n",
    "        optimizer = torch.optim.AdamW(self.parameters(),lr=LR)\n",
    "        return optimizer\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x,y = batch\n",
    "        out = self(x)\n",
    "        loss = F.cross_entropy(out,y)\n",
    "        self.log('train_loss', loss,on_step=True,on_epoch=True)\n",
    "        return loss \n",
    "\n",
    "    def test_step(self,batch,batch_idx):\n",
    "        x,y = batch\n",
    "        out = self(x)\n",
    "        loss = F.cross_entropy(out,y)\n",
    "        out = nn.Softmax(-1)(out) \n",
    "        logits = torch.argmax(out,dim=1)\n",
    "        accu = self.accuracy(logits, y)        \n",
    "        self.log('test_loss', loss)\n",
    "        self.log('train_acc_step', accu)\n",
    "        return loss, accu\n",
    "    \n",
    "    def validation_step(self,batch,batch_idx):\n",
    "        x,y = batch\n",
    "        out = self(x)\n",
    "        loss = F.cross_entropy(out,y)\n",
    "        out = nn.Softmax(-1)(out) \n",
    "        logits = torch.argmax(out,dim=1)\n",
    "        accu = self.accuracy(logits, y)        \n",
    "        self.log('Val_loss', loss)\n",
    "        self.log('Val_acc_step', accu)\n",
    "        return loss, accu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class load_CIFAR10data(pl.LightningDataModule):\n",
    "    def __init__(self, batch_size):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        transform = transforms.Compose([transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "        self.train = torchvision.datasets.CIFAR10(root='./CIFAR10_data', download=True ,train=True, transform=transform)\n",
    "        self.test = torchvision.datasets.CIFAR10(root='./CIFAR10_data', download=True ,train=False, transform=transform)\n",
    "        print('Data loaded')\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train, self.batch_size)    \n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test, self.batch_size)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.test, self.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "   | Name     | Type               | Params\n",
      "-------------------------------------------------\n",
      "0  | conv1    | Conv2d             | 896   \n",
      "1  | conv2    | Conv2d             | 9.2 K \n",
      "2  | fc1      | Linear             | 576 K \n",
      "3  | fc2      | Linear             | 5.0 K \n",
      "4  | relu1    | ReLU               | 0     \n",
      "5  | relu2    | ReLU               | 0     \n",
      "6  | relu3    | ReLU               | 0     \n",
      "7  | mxpool1  | MaxPool2d          | 0     \n",
      "8  | mxpool2  | MaxPool2d          | 0     \n",
      "9  | softmax  | LogSoftmax         | 0     \n",
      "10 | accuracy | MulticlassAccuracy | 0     \n",
      "-------------------------------------------------\n",
      "591 K     Trainable params\n",
      "0         Non-trainable params\n",
      "591 K     Total params\n",
      "2.367     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded\n",
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Local\\Miniconda\\envs\\Pytorch2\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 5000/5000 [01:39<00:00, 50.23it/s, v_num=54]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 5000/5000 [01:39<00:00, 50.21it/s, v_num=54]\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "     batch_size = 10\n",
    "     max_epochs = 1\n",
    "     num_classes = 10\n",
    "     data = load_CIFAR10data(batch_size)\n",
    "     mod = CIFAR10_mod(num_classes)\n",
    "     trainer = pl.Trainer(max_epochs=max_epochs)\n",
    "     trainer.fit(mod, data)\n",
    "     trainer.test(mod, data)\n",
    "\n",
    "if __name__ == '__main__': main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
